{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc713cda",
   "metadata": {
    "id": "bc713cda"
   },
   "source": [
    "# Implementing Decision tree from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218de310",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "218de310",
    "outputId": "93864191-1b7e-479f-bfd3-0759b4847266"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bebdb9f-26ff-4b24-8114-40346ab1dd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in /opt/conda/lib/python3.11/site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from ucimlrepo) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in /opt/conda/lib/python3.11/site-packages (from ucimlrepo) (2024.2.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n",
      "(48842, 15)\n",
      "{'uci_id': 2, 'name': 'Adult', 'repository_url': 'https://archive.ics.uci.edu/dataset/2/adult', 'data_url': 'https://archive.ics.uci.edu/static/public/2/data.csv', 'abstract': 'Predict whether income exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset. ', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 48842, 'num_features': 14, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Age', 'Income', 'Education Level', 'Other', 'Race', 'Sex'], 'target_col': ['income'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1996, 'last_updated': 'Mon Aug 07 2023', 'dataset_doi': '10.24432/C5XW20', 'creators': ['Barry Becker', 'Ronny Kohavi'], 'intro_paper': None, 'additional_info': {'summary': 'Extraction was done by Barry Becker from the 1994 Census database.  A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\\r\\n\\r\\nPrediction task is to determine whether a person makes over 50K a year.\\r\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Listing of attributes:\\r\\n\\r\\n>50K, <=50K.\\r\\n\\r\\nage: continuous.\\r\\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\\r\\nfnlwgt: continuous.\\r\\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\\r\\neducation-num: continuous.\\r\\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\\r\\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\\r\\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\\r\\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\\r\\nsex: Female, Male.\\r\\ncapital-gain: continuous.\\r\\ncapital-loss: continuous.\\r\\nhours-per-week: continuous.\\r\\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.', 'citation': None}}\n",
      "              name     role         type      demographic  \\\n",
      "0              age  Feature      Integer              Age   \n",
      "1        workclass  Feature  Categorical           Income   \n",
      "2           fnlwgt  Feature      Integer             None   \n",
      "3        education  Feature  Categorical  Education Level   \n",
      "4    education-num  Feature      Integer  Education Level   \n",
      "5   marital-status  Feature  Categorical            Other   \n",
      "6       occupation  Feature  Categorical            Other   \n",
      "7     relationship  Feature  Categorical            Other   \n",
      "8             race  Feature  Categorical             Race   \n",
      "9              sex  Feature       Binary              Sex   \n",
      "10    capital-gain  Feature      Integer             None   \n",
      "11    capital-loss  Feature      Integer             None   \n",
      "12  hours-per-week  Feature      Integer             None   \n",
      "13  native-country  Feature  Categorical            Other   \n",
      "14          income   Target       Binary           Income   \n",
      "\n",
      "                                          description units missing_values  \n",
      "0                                                 N/A  None             no  \n",
      "1   Private, Self-emp-not-inc, Self-emp-inc, Feder...  None            yes  \n",
      "2                                                None  None             no  \n",
      "3    Bachelors, Some-college, 11th, HS-grad, Prof-...  None             no  \n",
      "4                                                None  None             no  \n",
      "5   Married-civ-spouse, Divorced, Never-married, S...  None             no  \n",
      "6   Tech-support, Craft-repair, Other-service, Sal...  None            yes  \n",
      "7   Wife, Own-child, Husband, Not-in-family, Other...  None             no  \n",
      "8   White, Asian-Pac-Islander, Amer-Indian-Eskimo,...  None             no  \n",
      "9                                       Female, Male.  None             no  \n",
      "10                                               None  None             no  \n",
      "11                                               None  None             no  \n",
      "12                                               None  None             no  \n",
      "13  United-States, Cambodia, England, Puerto-Rico,...  None            yes  \n",
      "14                                       >50K, <=50K.  None             no  \n"
     ]
    }
   ],
   "source": [
    "!pip install ucimlrepo\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "# fetch dataset \n",
    "adult = fetch_ucirepo(id=2)   \n",
    "# data (as pandas dataframes) \n",
    "dataset = adult.data.original\n",
    "\n",
    "print(dataset.shape)\n",
    "# metadata \n",
    "print(adult.metadata) \n",
    "\n",
    "# variable information \n",
    "print(adult.variables) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fAWW5kChD6MU",
   "metadata": {
    "id": "fAWW5kChD6MU"
   },
   "outputs": [],
   "source": [
    "target_column = 'income'\n",
    "test_size = 0.2\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d77435f",
   "metadata": {
    "id": "3d77435f"
   },
   "outputs": [],
   "source": [
    "def remove_nans(df):\n",
    "    '''\n",
    "    this fucntion removes rows with nans \n",
    "    '''\n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "def numerical_to_categorical(df, n=2, ignore=[target_column]):\n",
    "    '''\n",
    "    change the type of the column to categorical \n",
    "    if it has <= n unique values\n",
    "    '''\n",
    "    # TODO\n",
    "    df_dropped =df.drop(columns=ignore, axis=1)\n",
    "    df_converted = df_dropped.apply(lambda x: x.astype('category')\\\n",
    "                                            if (str(x.dtype)=='int64' )\\\n",
    "                                            and (len(x.unique())<=n) else x)\n",
    "    for col in ignore:\n",
    "        df_converted[col] = df[col]\n",
    "    return df_converted\n",
    "\n",
    "\n",
    "def remove_columns_by_n(df, condition='pass', n=10, direction='less', \n",
    "                        ignore=[target_column]):\n",
    "    '''\n",
    "    Remove columns with more or less than n unique values. \n",
    "    Usually it makes sense to apply this function to columns with categorical values\n",
    "    (see below where it is called).\n",
    "    With the default values we remove all numerical columns which have\n",
    "    less than 10 unique values (except for the target_column).\n",
    "    '''\n",
    "    if ignore is None:\n",
    "        ignore = []\n",
    "    \n",
    "    # Ensure ignored columns are not processed\n",
    "    df_ignored = df.drop(columns=ignore, axis=1)\n",
    "    \n",
    "    columns_to_drop = []\n",
    "    for col in df_ignored.columns:\n",
    "        unique_count = len(df_ignored[col].unique())\n",
    "        if direction == 'less' and unique_count <= n and str(df_ignored[col].dtype) =='int64' :\n",
    "            columns_to_drop.append(col)\n",
    "        elif direction == 'more' and unique_count >= n and str(df_ignored[col].dtype) =='category':\n",
    "            columns_to_drop.append(col)\n",
    "    \n",
    "    df_dropped = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Adding the ignored columns back if they were dropped\n",
    "    for col in ignore:\n",
    "        if col in df.columns:\n",
    "            df_dropped[col] = df[col]\n",
    "    \n",
    "    return df_dropped\n",
    "\n",
    "\n",
    "def object_to_categorical(df):\n",
    "    '''\n",
    "    Make columns with the 'object' type categorical \n",
    "    and replace categories with label encodings\n",
    "    '''\n",
    "    # TODO\n",
    "    return df.apply(lambda x : x.astype('category') if str(x.dtype)=='object' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9033619f",
   "metadata": {
    "id": "9033619f"
   },
   "outputs": [],
   "source": [
    "df = dataset\n",
    "df = remove_nans(df)\n",
    "#TODO-SELF:in ds some classes has dot at the end i.e <=50K. that makes to have 4 unique values\n",
    "#but we should have 2 because our target is binary\n",
    "df.loc[:, target_column] = df[target_column].apply(lambda x: x.replace('.', '') if isinstance(x, str) and '.' in x else x)\n",
    "df = numerical_to_categorical(df, n=2, ignore=[target_column])\n",
    "df = remove_columns_by_n(df, n=10, direction='less', ignore=[target_column], condition='pass')\n",
    "df = object_to_categorical(df)\n",
    "df = remove_columns_by_n(df, n=40, direction='more',ignore=[target_column], condition='pass')\n",
    "# #TODO-SELF: label encoding \n",
    "from sklearn.preprocessing import LabelEncoder                        \n",
    "encoder = LabelEncoder()\n",
    "# Encode the categorical column\n",
    "df=df.apply(lambda x: encoder.fit_transform(x))\n",
    "assert not df.isna().any().any(), 'There are still nans in the dataframe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22348559",
   "metadata": {
    "id": "22348559"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47621, 13)\n"
     ]
    }
   ],
   "source": [
    "# TODO : make train-test split from the dataframe using the parameters above\n",
    "# expected results variable names - train_X, test_X, train_y, test_y\n",
    "\n",
    "X = df.loc[:, df.columns != target_column]\n",
    "y = df[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    ")\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dee4eeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples = 100000 # our implementation took ~1 min with this amount of samples, you can reduce the number if neccessary\n",
    "n_samples = 10000\n",
    "X_train_tree, X_test_tree, y_train_tree, y_test_tree = X_train.to_numpy()[:n_samples], X_test.to_numpy(), y_train.to_numpy()[:n_samples], y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b28e04c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y, labels=(0, 1)):\n",
    "    gini=0\n",
    "    for k in labels:\n",
    "         #calculate proportion\n",
    "        indicator = y == k\n",
    "        #count I(tn =k)\n",
    "        true_cases = np.sum(indicator)\n",
    "        p = true_cases / len(y)\n",
    "        gini += p*(1-p)\n",
    "    return gini\n",
    "        \n",
    "\n",
    "def weighted_difference(y, split, labels=(0, 1)):\n",
    "    #weighted average\n",
    "    l_a_weight = (np.sum(split)/len(y))\n",
    "    #gini of left side of split\n",
    "    gini_minus = gini_impurity(y[split])\n",
    "    # not symbol ~ corresponds the other side of split\n",
    "    l_b_weight = (np.sum(~split)/len(y))\n",
    "    gini_posit = gini_impurity(y[~split])\n",
    "    weighted_diff = gini_impurity(y)-(l_a_weight*gini_minus)-(l_b_weight*gini_posit)\n",
    "    import math\n",
    "    return weighted_diff if not math.isnan(weighted_diff )else float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29844df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36417528\n",
      "0.0005766475406871335\n"
     ]
    }
   ],
   "source": [
    "print(gini_impurity(y_train_tree, labels=(0, 1)))\n",
    "\n",
    "split = X_train_tree[:,0] < 40\n",
    "print(weighted_difference(y_train_tree, split))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f06029fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1523/1353180847.py:8: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  p = true_cases / len(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 1)\n"
     ]
    }
   ],
   "source": [
    "# for each new node to be considred we calculate the exaustive search for the node , and i guess for the last node leading\n",
    "# to leafs we just calculate gini_impurity\n",
    "def exhaustive_search(x, y):\n",
    "    '''\n",
    "        for each node it provides full list of possible midpoints and their impurities\n",
    "    '''\n",
    "    # midpoints (potential split points)\n",
    "    midpoint_ls = [np.unique(x[:,idx]) for idx in range(x.shape[1])]\n",
    "\n",
    "    # which feature each midpoints/impurities corresponds to.\n",
    "    feature_array = np.hstack([np.full(len(midpt_per_feat), idx) for idx,midpt_per_feat in enumerate(midpoint_ls)])\n",
    "    \n",
    "    #numpying and flattening midpoints_ls  \n",
    "    midpoint_array = np.hstack(midpoint_ls)\n",
    "    \n",
    "    #impurity reduction values corresponding to each midpoint.        \n",
    "    impurity_array = np.array([weighted_difference(y,x[:,feature_array[idx]] >= midpoint)\n",
    "                              for idx,midpoint in enumerate(midpoint_array)],dtype='float32')\n",
    "             \n",
    "    return feature_array,midpoint_array,impurity_array\n",
    "                                  \n",
    "feat_arr,midpoint_arr,impurt_arr = exhaustive_search(X_train_tree,y_train_tree)\n",
    "\n",
    "def find_best_split(impurities_array, midpoints_array, features_array, verbose=False):\n",
    "    \n",
    "    #idx of best impurity reduction score\n",
    "    optimal_idx = np.argmax(impurities_array)\n",
    "    feature_idx = features_array[optimal_idx]\n",
    "    best_midpoint = midpoints_array[optimal_idx]\n",
    "    return (feature_idx,best_midpoint)\n",
    "    \n",
    "best_split= find_best_split(impurt_arr, midpoint_arr, feat_arr, verbose=False)\n",
    "print(best_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81924410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, left, right, n_feat, threshold):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.n_feat = n_feat #prpbablity it means the idx of feature chosen\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, label):\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ef87849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement the function build_tree and predict_tree\n",
    "\n",
    "# Implement recursive tree function\n",
    "#QUESTIONS: what is the output of buildtree ? i think it may be a Node object which has nested nodes and leaf (i.e in self.left / self.right)\n",
    "def build_tree(x, y, current_depth, max_depth=3, n_labels=2):\n",
    "    if current_depth >= max_depth:\n",
    "        leaf =Leaf(np.argmax(np.bincount(y)))\n",
    "        return leaf\n",
    "        \n",
    "    feat_arr,midpoint_arr,impurt_arr = exhaustive_search(x,y)\n",
    "    feat_idx,threshold = find_best_split(impurt_arr, midpoint_arr, feat_arr)\n",
    "    print(feat_idx,threshold)\n",
    "    split_mask = x[:,feat_idx]<threshold\n",
    "    \n",
    "    left_x = x[split_mask]\n",
    "    left_y = y[split_mask]\n",
    "    right_x = x[~split_mask]\n",
    "    right_y = y[~split_mask]\n",
    "    tree = Node(build_tree(left_x, left_y, current_depth=current_depth+1),\n",
    "                build_tree(right_x, right_y, current_depth=current_depth+1),feat_idx,threshold)\n",
    "    return tree\n",
    "    \n",
    "def show_tree(tree,):\n",
    "    'DFS based construction of tree'\n",
    "    pass\n",
    "\n",
    "\n",
    "def predict_tree(node, x):\n",
    "    while True:\n",
    "        if node.__class__.__name__ == 'Leaf':\n",
    "            return node.label\n",
    "            \n",
    "        feat_idx,threshold = node.n_feat,node.threshold\n",
    "        is_left = x[feat_idx] <threshold\n",
    "        if is_left :\n",
    "            node = node.left\n",
    "            continue\n",
    "        node = node.right\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "547ca58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1523/1353180847.py:8: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  p = true_cases / len(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 1\n",
      "4 12\n",
      "10 81\n",
      "10 81\n",
      "10 94\n",
      "7 5\n",
      "0 6\n"
     ]
    }
   ],
   "source": [
    "# Build tree\n",
    "tree = build_tree(X_train_tree,y_train_tree,0)\n",
    "\n",
    "apply_pred = lambda x_dpoint : predict_tree(tree, x_dpoint)\n",
    "predictions_train = np.apply_along_axis(apply_pred, axis=1, arr=X_train_tree)\n",
    "predictions_test = np.apply_along_axis(apply_pred, axis=1, arr=X_test_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f93b1a5e-67d4-4d22-8098-46c352f54eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn decision tree\n",
    "sk_tree = DecisionTreeClassifier(max_depth=3).fit(X_train_tree,y_train_tree)\n",
    "sk_predictions_train = sk_tree.predict(X_train_tree)\n",
    "sk_predictions_test = sk_tree.predict(X_test_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6da6847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------MY MODEL -----------------\n",
      "Accuracy Training:  0.8408\n",
      "Accuracy:  0.8371653543307087\n",
      "Precision:  0.80782704980924\n",
      "Recall:  0.7130646540555481\n",
      "------------------SK LEARN -----------------\n",
      "Accuracy Training:  0.8408\n",
      "Accuracy:  0.8372703412073491\n",
      "Precision:  0.8079616592624852\n",
      "Recall:  0.7132812918371773\n"
     ]
    }
   ],
   "source": [
    "# Calculate training and test scores\n",
    "print('------------------MY MODEL -----------------')\n",
    "print('Accuracy Training: ', accuracy_score(y_train_tree, predictions_train))\n",
    "print('Accuracy: ', accuracy_score(y_test_tree, predictions_test))\n",
    "print('Precision: ', precision_score(y_test_tree, predictions_test, average='macro'))\n",
    "print('Recall: ', recall_score(y_test_tree, predictions_test, average='macro'))\n",
    "print('------------------SK LEARN -----------------')\n",
    "print('Accuracy Training: ', accuracy_score(y_train_tree, sk_predictions_train))\n",
    "print('Accuracy: ', accuracy_score(y_test_tree, sk_predictions_test))\n",
    "print('Precision: ', precision_score(y_test_tree, sk_predictions_test, average='macro'))\n",
    "print('Recall: ', recall_score(y_test_tree, sk_predictions_test, average='macro'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
