{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc713cda",
   "metadata": {
    "id": "bc713cda"
   },
   "source": [
    "# Decision tree from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "218de310",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "218de310",
    "outputId": "93864191-1b7e-479f-bfd3-0759b4847266"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder     \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score,mean_absolute_error\n",
    "\n",
    "from ucimlrepo import fetch_ucirepo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bebdb9f-26ff-4b24-8114-40346ab1dd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 2, 'name': 'Adult', 'repository_url': 'https://archive.ics.uci.edu/dataset/2/adult', 'data_url': 'https://archive.ics.uci.edu/static/public/2/data.csv', 'abstract': 'Predict whether income exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset. ', 'area': 'Social Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 48842, 'num_features': 14, 'feature_types': ['Categorical', 'Integer'], 'demographics': ['Age', 'Income', 'Education Level', 'Other', 'Race', 'Sex'], 'target_col': ['income'], 'index_col': None, 'has_missing_values': 'yes', 'missing_values_symbol': 'NaN', 'year_of_dataset_creation': 1996, 'last_updated': 'Mon Aug 07 2023', 'dataset_doi': '10.24432/C5XW20', 'creators': ['Barry Becker', 'Ronny Kohavi'], 'intro_paper': None, 'additional_info': {'summary': 'Extraction was done by Barry Becker from the 1994 Census database.  A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\\r\\n\\r\\nPrediction task is to determine whether a person makes over 50K a year.\\r\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'Listing of attributes:\\r\\n\\r\\n>50K, <=50K.\\r\\n\\r\\nage: continuous.\\r\\nworkclass: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\\r\\nfnlwgt: continuous.\\r\\neducation: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\\r\\neducation-num: continuous.\\r\\nmarital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\\r\\noccupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\\r\\nrelationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\\r\\nrace: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\\r\\nsex: Female, Male.\\r\\ncapital-gain: continuous.\\r\\ncapital-loss: continuous.\\r\\nhours-per-week: continuous.\\r\\nnative-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.', 'citation': None}}\n"
     ]
    }
   ],
   "source": [
    "# fetch adult dataset \n",
    "adult = fetch_ucirepo(id=2) \n",
    "dataset = adult.data.original\n",
    "print(adult.metadata) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d77435f",
   "metadata": {
    "id": "3d77435f"
   },
   "outputs": [],
   "source": [
    "def remove_nans(df):\n",
    "    return df.dropna()\n",
    "\n",
    "\n",
    "def convert_num_to_cat(df, n=2, ignore=None):\n",
    "    '''\n",
    "    Type conversion based on n number of column unique values \n",
    "    '''\n",
    "    df_dropped =df.drop(columns=ignore, axis=1)\n",
    "    df_converted = df_dropped.apply(lambda x: x.astype('category')\\\n",
    "                                            if (str(x.dtype)=='int64' )\\\n",
    "                                            and (len(x.unique())<=n) else x)\n",
    "    for col in ignore:\n",
    "        df_converted[col] = df[col]\n",
    "    return df_converted\n",
    "\n",
    "\n",
    "def remove_columns(df, n=10, direction='less',ignore=None):\n",
    "    '''\n",
    "    Remove columns applying condition on n unique values. \n",
    "    '''\n",
    "    if ignore is None:\n",
    "        ignore = []\n",
    "    \n",
    "    # Ensure ignored columns are not processed\n",
    "    df_ignored = df.drop(columns=ignore, axis=1)\n",
    "    \n",
    "    columns_to_drop = []\n",
    "    for col in df_ignored.columns:\n",
    "        unique_count = len(df_ignored[col].unique())\n",
    "        if direction == 'less' and unique_count <= n and str(df_ignored[col].dtype) =='int64' :\n",
    "            columns_to_drop.append(col)\n",
    "        elif direction == 'more' and unique_count >= n and str(df_ignored[col].dtype) =='category':\n",
    "            columns_to_drop.append(col)\n",
    "    \n",
    "    df_dropped = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    # Adding the ignored columns back if they were dropped\n",
    "    for col in ignore:\n",
    "        if col in df.columns:\n",
    "            df_dropped[col] = df[col]\n",
    "    \n",
    "    return df_dropped\n",
    "\n",
    "\n",
    "def object_to_categorical(df):\n",
    "    '''\n",
    "    type conversion 'object' to categorical  \n",
    "    '''\n",
    "    return df.apply(lambda x : x.astype('category') if str(x.dtype)=='object' else x)\n",
    "\n",
    "def encode_cat(df):          \n",
    "    encoder = LabelEncoder()\n",
    "    return df.apply(lambda x: encoder.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fAWW5kChD6MU",
   "metadata": {
    "id": "fAWW5kChD6MU"
   },
   "outputs": [],
   "source": [
    "TARGET_COLUMN = 'income'\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "N_SAMPLES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9033619f",
   "metadata": {
    "id": "9033619f"
   },
   "outputs": [],
   "source": [
    "# Preprocessing \n",
    "\n",
    "df = dataset\n",
    "\n",
    "df = remove_nans(df)\n",
    "\n",
    "#some classes has dot at the end i.e <=50K. \n",
    "df.loc[:, TARGET_COLUMN] = df[TARGET_COLUMN].apply(lambda x: x.replace('.', '') if isinstance(x, str) and '.' in x else x)\n",
    "\n",
    "df = convert_num_to_cat(df, n=2, ignore=[TARGET_COLUMN])\n",
    "df = remove_columns(df, n=10, direction='less', ignore=[TARGET_COLUMN])\n",
    "df = object_to_categorical(df)\n",
    "df = remove_columns(df, n=40, direction='more',ignore=[TARGET_COLUMN])\n",
    "df = encode_cat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "22348559",
   "metadata": {
    "id": "22348559"
   },
   "outputs": [],
   "source": [
    "# Train-Test split \n",
    "X = df.loc[:, df.columns != TARGET_COLUMN]\n",
    "y = df[TARGET_COLUMN]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
    "#sampling \n",
    "X_train_np, X_test_np, y_train_np, y_test_np = X_train.to_numpy()[:N_SAMPLES], X_test.to_numpy(), y_train.to_numpy()[:N_SAMPLES], y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b28e04c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y, labels=(0, 1)):\n",
    "    if len(y)==0:\n",
    "        #controlling divided by zero\n",
    "        return 1\n",
    "    gini=0\n",
    "    for k in labels:\n",
    "        # true labeled mask\n",
    "        indicator = y == k\n",
    "        true_cases = np.sum(indicator)\n",
    "        p = true_cases / len(y)\n",
    "        gini += p*(1-p)\n",
    "    return gini \n",
    "        \n",
    "\n",
    "def gini_weighted_difference(y, split, labels=(0, 1)):\n",
    "    #weighted average\n",
    "    l_a_weight = (np.sum(split)/len(y))\n",
    "    l_b_weight = (np.sum(~split)/len(y))\n",
    "    \n",
    "    # each split gini impurity\n",
    "    gini_minus = gini_impurity(y[split])\n",
    "    gini_posit = gini_impurity(y[~split])\n",
    "\n",
    "    weighted_diff = gini_impurity(y)-(l_a_weight*gini_minus)-(l_b_weight*gini_posit)\n",
    "    \n",
    "    return weighted_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f06029fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(x, y):\n",
    "    '''\n",
    "        for each node it provides full list of possible midpoints and their impurities\n",
    "    '''\n",
    "    # midpoints (potential split points)\n",
    "    midpoint_ls = [np.unique(x[:,idx]) for idx in range(x.shape[1])]\n",
    "\n",
    "    # which feature each midpoints/impurities corresponds to.\n",
    "    feature_array = np.hstack([np.full(len(midpt_per_feat), idx) for idx,midpt_per_feat in enumerate(midpoint_ls)])\n",
    "    \n",
    "    #numpying and flattening midpoints_ls  \n",
    "    midpoint_array = np.hstack(midpoint_ls)\n",
    "    \n",
    "    #impurity reduction values corresponding to each midpoint.        \n",
    "    impurity_array = np.array([gini_weighted_difference(y,x[:,feature_array[idx]] >= midpoint)\n",
    "                              for idx,midpoint in enumerate(midpoint_array)],dtype='float32')\n",
    "             \n",
    "    return feature_array,midpoint_array,impurity_array\n",
    "                                  \n",
    "\n",
    "def find_best_split(impurities_array, midpoints_array, features_array, verbose=False):\n",
    "    \n",
    "    #idx of best impurity reduction score\n",
    "    optimal_idx = np.argmax(impurities_array)\n",
    "    feature_idx = features_array[optimal_idx]\n",
    "    best_midpoint = midpoints_array[optimal_idx]\n",
    "\n",
    "    return (feature_idx,best_midpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "81924410",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, left, right, feat_idx, split_point):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.feat_idx = feat_idx #prpbablity it means the idx of feature chosen\n",
    "        self.split_point = split_point\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__} x[:,{self.feat_idx}]<={self.split_point}'\n",
    "\n",
    "\n",
    "class Leaf:\n",
    "    def __init__(self, label):\n",
    "        self.label = label\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'{self.__class__.__name__} {self.label}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1ef87849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(x, y, current_depth, max_depth=3, n_labels=2):\n",
    "    if current_depth >= max_depth:\n",
    "        leaf =Leaf(np.argmax(np.bincount(y)))\n",
    "        return leaf\n",
    "        \n",
    "    feat_arr,midpoint_arr,impurt_arr = grid_search(x,y)\n",
    "\n",
    "    feat_idx,split_point = find_best_split(impurt_arr, midpoint_arr, feat_arr)\n",
    "\n",
    "    split_mask = x[:,feat_idx]<split_point\n",
    "    left_x = x[split_mask]\n",
    "    left_y = y[split_mask]\n",
    "    right_x = x[~split_mask]\n",
    "    right_y = y[~split_mask]\n",
    "\n",
    "    tree = Node(build_tree(left_x, left_y, current_depth=current_depth+1) if left_y.size !=0 else None,\n",
    "                build_tree(right_x, right_y, current_depth=current_depth+1)if right_y.size !=0 else None,feat_idx,split_point)\n",
    "    print(tree.left)\n",
    "    return tree\n",
    "    \n",
    "def show_tree(node):\n",
    "    pass\n",
    "    # TODO: implem DFS based \n",
    "\n",
    "\n",
    "def predict(node, x):\n",
    "    while True:\n",
    "        if node.__class__.__name__ == 'Leaf':\n",
    "            return node.label\n",
    "            \n",
    "        feat_idx,split_point = node.feat_idx,node.split_point\n",
    "        is_left = x[feat_idx] <split_point\n",
    "        if is_left :\n",
    "            node = node.left\n",
    "            continue\n",
    "        node = node.right\n",
    "        \n",
    "def predict_all(tree,X):\n",
    "    apply_pred = lambda x_dpoint : predict(tree, x_dpoint)\n",
    "    return np.apply_along_axis(apply_pred, axis=1, arr=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529a7b6f",
   "metadata": {},
   "source": [
    "## Comparing with Scikit learn implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "547ca58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaf 0\n",
      "Leaf 1\n",
      "Node x[:,10]<=81\n",
      "Leaf 0\n",
      "Leaf 0\n",
      "Node x[:,7]<=5\n",
      "Node x[:,4]<=12\n"
     ]
    }
   ],
   "source": [
    "# Build vanilla decision tree\n",
    "tree = build_tree(X_train_np,y_train_np,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9b56d30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "#predict with vanilla tree\n",
    "predictions_train = predict_all(tree,X_train_np)\n",
    "predictions_test = predict_all(tree,X_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f93b1a5e-67d4-4d22-8098-46c352f54eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn decision tree\n",
    "sk_tree = DecisionTreeClassifier(max_depth=3).fit(X_train_np,y_train_np)\n",
    "sk_predictions_train = sk_tree.predict(X_train_np)\n",
    "sk_predictions_test = sk_tree.predict(X_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d6da6847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------MY MODEL -----------------\n",
      "Accuracy Training:  0.8380144897102058\n",
      "Accuracy:  0.8372703412073491\n",
      "Precision:  0.8079616592624852\n",
      "Recall:  0.7132812918371773\n",
      "------------------MY MODEL -----------------\n",
      "Accuracy Training:  0.8380144897102058\n",
      "Accuracy:  0.8372703412073491\n",
      "Precision:  0.8079616592624852\n",
      "Recall:  0.7132812918371773\n"
     ]
    }
   ],
   "source": [
    "# Calculate training and test scores\n",
    "vanilla_acc_score_train =accuracy_score(y_train_np, predictions_train)\n",
    "vanilla_acc_score_test =accuracy_score(y_test_np, predictions_test)\n",
    "vanilla_preci_score =precision_score(y_test_np, predictions_test, average='macro')\n",
    "vanilla_recall_score = recall_score(y_test_np, predictions_test, average='macro')\n",
    "print('------------------MY MODEL -----------------')\n",
    "print('Accuracy Training: ',vanilla_acc_score_train)\n",
    "print('Accuracy: ',vanilla_acc_score_test)\n",
    "print('Precision: ',vanilla_preci_score)\n",
    "print('Recall: ',vanilla_recall_score)\n",
    "\n",
    "sk_acc_score_train =accuracy_score(y_train_np, predictions_train)\n",
    "sk_acc_score_test =accuracy_score(y_test_np, predictions_test)\n",
    "sk_preci_score =precision_score(y_test_np, predictions_test, average='macro')\n",
    "sk_recall_score = recall_score(y_test_np, predictions_test, average='macro')\n",
    "print('------------------MY MODEL -----------------')\n",
    "print('Accuracy Training: ',sk_acc_score_train)\n",
    "print('Accuracy: ',sk_acc_score_test)\n",
    "print('Precision: ',sk_preci_score)\n",
    "print('Recall: ',sk_recall_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "22ae91b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# measuring similarity between metrics of vanilla model and sk learn using relative difference minues one\n",
    "sim_acc_train = 1-(((vanilla_acc_score_train-sk_acc_score_train)**2)/sk_acc_score_train)\n",
    "sim_acc_test = 1-(((vanilla_acc_score_test-sk_acc_score_test)**2)/sk_acc_score_test)\n",
    "sim_preci = 1-(((vanilla_preci_score-sk_preci_score)**2)/sk_preci_score)\n",
    "sim_recall = 1-(((vanilla_recall_score-sk_recall_score)**2)/sk_recall_score)\n",
    "\n",
    "print(sim_acc_test,sim_acc_train,sim_preci,sim_recall)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
